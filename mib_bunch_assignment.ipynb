{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import os\n",
    "import src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk size\n",
    "chunk_size = 1000\n",
    "\n",
    "# make a histogram with n bins\n",
    "n_bins = 500\n",
    "\n",
    "#tof scaling\n",
    "tof_scaling = 1e-9\n",
    "\n",
    "#q scaling: from GeV to eV, from eV to e (3.6eV / e- in Si)\n",
    "q_scaling = 1e9/3.67\n",
    "\n",
    "#q threshold\n",
    "q_threshold = 1000\n",
    "\n",
    "# window size: extend by this many in both directions\n",
    "shift_n_times = 0\n",
    "\n",
    "# tof offset in [ns]\n",
    "shift_offset = 0\n",
    "\n",
    "# print infos during run\n",
    "verbose = False\n",
    "\n",
    "# width of tornado mask [ns]\n",
    "mask_xwidth = 25\n",
    "\n",
    "#shift mask along x axis [ns]\n",
    "mask_xoffset = -5 \n",
    "\n",
    "#set to -1 to mirror curves against time; stretch mask along x axis\n",
    "mask_xscale = -1 \n",
    "\n",
    "# to handle numerical rounding errors\n",
    "epsilon = 1e-8\n",
    "\n",
    "# remote path to hdf5\n",
    "remote_path = \"../../../sshfs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(remote_path+'tof_q.h5', 'r')\n",
    "\n",
    "#get group\n",
    "hf_group = hf.get('Tof_q')\n",
    "hf_group.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dset = hf_group.get(\"kaons+-\")\n",
    "hf_dset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tof_low = np.inf\n",
    "tof_high = -np.inf\n",
    "q_low = np.inf\n",
    "q_high = -np.inf\n",
    "\n",
    "# chunks size to read data in chunk_size chunks\n",
    "n_chunks = int(np.ceil(len(hf_dset)/chunk_size))\n",
    "\n",
    "#find the overall min/max\n",
    "# x axis in [ns] (*1e-9) y axis in [GeV] (1e-3)\n",
    "for chunk_idx in range(n_chunks):\n",
    "    print(\"[{}/{}]\".format(chunk_idx+1, n_chunks))\n",
    "    if verbose:\n",
    "        print(\"tof min: {}, tof max: {}\".format(tof_low, tof_high))\n",
    "        print(\"q min: {}, q max: {}\".format(q_low, q_high))\n",
    "    tof_low = np.minimum(hf_dset[chunk_idx*chunk_size:(chunk_idx+1)*chunk_size, 0].min(), tof_low)\n",
    "    tof_high = np.maximum(hf_dset[chunk_idx*chunk_size:(chunk_idx+1)*chunk_size, 0].max(), tof_high)\n",
    "    q_low = np.minimum(hf_dset[chunk_idx*chunk_size:(chunk_idx+1)*chunk_size, 1].min(), q_low)\n",
    "    q_high = np.maximum(hf_dset[chunk_idx*chunk_size:(chunk_idx+1)*chunk_size, 1].max(), q_high)\n",
    "    \n",
    "# extend tof range to copy data\n",
    "tof_high += shift_n_times*shift_offset\n",
    "tof_low -= shift_n_times*shift_offset\n",
    "\n",
    "# scale bin limits\n",
    "tof_low = tof_low*tof_scaling\n",
    "tof_high = tof_high*tof_scaling\n",
    "q_low = q_low*q_scaling\n",
    "q_high = q_high*q_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create bin limits based on min/max values, and scale the values\n",
    "tof_bin_edges = np.linspace(tof_low-epsilon, tof_high+epsilon, (2*shift_n_times+1)*n_bins+1)\n",
    "q_bin_edges = np.linspace(q_low-epsilon, q_high+epsilon, n_bins+1)\n",
    "\n",
    "#create empty 2d histograms\n",
    "tof_histo = np.zeros((n_bins, (2*shift_n_times+1)*n_bins))\n",
    "tof_below_histo = np.zeros((n_bins, (2*shift_n_times+1)*n_bins))\n",
    "tof_above_histo = np.zeros((n_bins, (2*shift_n_times+1)*n_bins))\n",
    "\n",
    "# iterate over the dataset in chunks of n_chunks lines\n",
    "for chunk_idx in range(n_chunks):\n",
    "    print(\"[{}/{}]\".format(chunk_idx+1, n_chunks))\n",
    "    \n",
    "    # x axis in [ns] (*1e-9) y axis in [GeV] (1e-3)\n",
    "    tof_chunk = hf_dset[chunk_idx*chunk_size:(chunk_idx+1)*chunk_size, 0]\n",
    "    q_chunk = hf_dset[chunk_idx*chunk_size:(chunk_idx+1)*chunk_size, 1]\n",
    "    if verbose:\n",
    "        print(\"New data chunk length: {}\".format(len(tof_chunk)))\n",
    "    \n",
    "    # create data copies (offset value in [ns])\n",
    "    tof_chunk_extended = np.copy(tof_chunk)\n",
    "    q_chunk_extended = np.copy(q_chunk)\n",
    "    for i in range(1, shift_n_times+1):\n",
    "        tof_chunk_extended = np.concatenate((tof_chunk_extended, tof_chunk+i*shift_offset, tof_chunk-i*shift_offset))\n",
    "        q_chunk_extended = np.concatenate((q_chunk_extended, q_chunk, q_chunk))    \n",
    "    tof_chunk = tof_chunk_extended\n",
    "    q_chunk = q_chunk_extended\n",
    "    if verbose:\n",
    "        print(\"Extended data chunk length: {}\".format(len(tof_chunk)))\n",
    "    \n",
    "    # scale data\n",
    "    tof_chunk *= tof_scaling\n",
    "    q_chunk *= q_scaling\n",
    "    \n",
    "    # split data along q threshold\n",
    "    tof_chunk_below = tof_chunk[q_chunk < q_threshold]\n",
    "    q_chunk_below = q_chunk[q_chunk < q_threshold]\n",
    "    tof_chunk_above = tof_chunk[q_chunk >= q_threshold]\n",
    "    q_chunk_above = q_chunk[q_chunk >= q_threshold]\n",
    "    \n",
    "    #fill 2d histo with data chunk\n",
    "    tof_chunk_histo, xedges, yedges = np.histogram2d(\n",
    "        tof_chunk, q_chunk, bins=(tof_bin_edges, q_bin_edges));\n",
    "    tof_chunk_below_histo, xedges_below, yedges_below = np.histogram2d(\n",
    "        tof_chunk_below, q_chunk_below, bins=(tof_bin_edges, q_bin_edges));\n",
    "    tof_chunk_above_histo, xedges_above, yedges_above = np.histogram2d(\n",
    "        tof_chunk_above, q_chunk_above, bins=(tof_bin_edges, q_bin_edges));\n",
    "    \n",
    "    # accumulate bin counts over chunks\n",
    "    tof_histo += tof_chunk_histo.T\n",
    "    tof_below_histo += tof_chunk_below_histo.T\n",
    "    tof_above_histo += tof_chunk_above_histo.T\n",
    "    print(\"Hits added to histo so far: {}\".format(int(np.sum(tof_histo))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.ma.masked_where(tof_histo==0, tof_histo), interpolation='nearest', origin='lower',\n",
    "        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]], aspect=\"auto\", vmin=0, vmax=1000)\n",
    "plt.grid()\n",
    "plt.xlabel(\"ToF [s]\", fontsize=16)\n",
    "plt.ylabel(\"Q [e-]\", fontsize=16)\n",
    "#plt.ylim(0, .001*q_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load timewalk sim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read timewalk sim\n",
    "data = pd.read_csv(\"../tornado_plot/timewalk_Qth1000_RD53B.csv\", header=0)\n",
    "\n",
    "# insert a 0th and a n+1th row to facilitate thresholding at 1000 e-\n",
    "data = pd.concat([pd.DataFrame(data.iloc[0]).T, data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read timewalk sim\n",
    "data = pd.read_csv(\"../tornado_plot/timewalk_Qth1000_RD53B.csv\", header=0)\n",
    "\n",
    "# insert a 0th and a n+1th row to facilitate thresholding at 1000 e-\n",
    "data = pd.concat([pd.DataFrame(data.iloc[0]).T, data]).reset_index(drop=True)\n",
    "data.at[0, \"Timewalk\"] = data.at[0, \"Timewalk\"] + mask_xwidth*tof_scaling\n",
    "data = pd.concat([data, pd.DataFrame(data.iloc[-1]).T]).reset_index(drop=True)\n",
    "data.at[len(data)-1, \"Qel \"] = data.at[len(data)-1, \"Qel \"]*2\n",
    "\n",
    "#scale mask e.g. mirror in time\n",
    "data[\"Timewalk\"] = mask_xscale*data[\"Timewalk\"] + mask_xoffset*tof_scaling\n",
    "\n",
    "#shift curve to get other boundary\n",
    "data[\"timewalk_shifted\"] = data[\"Timewalk\"] + mask_xwidth*tof_scaling\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(-shift_n_times, shift_n_times+1):\n",
    "    l1 = \"simulation\" if i == 0 else \"\"\n",
    "    l2 = \"shifted by 25ns\" if i == 0 else \"\"\n",
    "    plt.plot(data[\"Timewalk\"]+i*shift_offset*tof_scaling, data[\"Qel \"], label=l1, c=\"b\")\n",
    "    plt.plot(data[\"timewalk_shifted\"]+i*shift_offset*tof_scaling, data[\"Qel \"], label=l2, c=\"r\")\n",
    "plt.xlabel(\"Time of flight [s]\")\n",
    "plt.ylabel(\"Q [e-]\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlay sim curve and bib data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mask, Y_mask = np.meshgrid(tof_bin_edges, q_bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(tof_histo==0, tof_histo))\n",
    "#plt.ylim(0, .01*q_scaling)\n",
    "plt.grid()\n",
    "\n",
    "# tornado mask curves\n",
    "plt.plot(data[\"Timewalk\"], data[\"Qel \"], label=\"simulation\")\n",
    "plt.plot(data[\"timewalk_shifted\"], data[\"Qel \"], label=\"shifted\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time of flight [ns]\", fontsize=16)\n",
    "plt.ylabel(\"Q [e-]\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate geometry way to calculate area below curve\n",
    "color scatter\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cx_list = []\n",
    "cy_list = []\n",
    "\n",
    "cell_mask = np.zeros((n_bins, (2*shift_n_times+1)*n_bins))#*np.nan\n",
    "\n",
    "#loop goes from 0 to nbins, starts from bottom left corner\n",
    "for yi in range(n_bins):\n",
    "    print(\"[{}/{}]\".format(yi+1, n_bins))\n",
    "    for xi in range((2*shift_n_times+1)*n_bins):\n",
    "        xmin, xmax = xedges[xi], xedges[xi+1]\n",
    "        ymin, ymax = yedges[yi], yedges[yi+1]\n",
    "        cell_area = (xmax - xmin)*(ymax - ymin)            \n",
    "        cx = (xmax + xmin)/2\n",
    "        cy = (ymax + ymin)/2\n",
    "\n",
    "        #get cell area below all curves\n",
    "        area_below_left_curve = []\n",
    "        area_below_right_curve = []\n",
    "        for i in range(-shift_n_times, shift_n_times+1):\n",
    "            if verbose:\n",
    "                print(\"working on mask copy {}\".format(i))\n",
    "            area_below_left_curve.append(src.getAreaBelowCurve(xmin, xmax, ymin, ymax, cx, cy,\n",
    "                                        data[\"Timewalk\"]+i*shift_offset*tof_scaling, data[\"Qel \"], verbose=verbose))\n",
    "            area_below_right_curve.append(src.getAreaBelowCurve(xmin, xmax, ymin, ymax, cx, cy,\n",
    "                                        data[\"timewalk_shifted\"]+i*shift_offset*tof_scaling, data[\"Qel \"], verbose=verbose))\n",
    "        \n",
    "        # cell mask value is between 0 and 1\n",
    "        area_between_curves = np.sum(area_below_left_curve) - np.sum(area_below_right_curve)\n",
    "        cell_mask_value = area_between_curves/cell_area\n",
    "        \n",
    "        #fill up mask array with cell mask value\n",
    "        if cell_mask_value < epsilon:\n",
    "            cell_mask_value = 0\n",
    "        else:\n",
    "            \n",
    "            #store cells that are between two curves\n",
    "            cx_list.append(cx)\n",
    "            cy_list.append(cy)\n",
    "            \n",
    "            #numerics correction\n",
    "            if cell_mask_value > 1:\n",
    "                cell_mask_value = 1\n",
    "            if np.abs(cell_mask_value - 1) < epsilon:\n",
    "                cell_mask_value = 1   \n",
    "  \n",
    "            #save cell masks into numpy array (start from top left corner)\n",
    "            cell_mask[yi, xi] = cell_mask_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "data_plot = plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(tof_histo==0, tof_histo),\n",
    "                           vmin=1, vmax=np.max(tof_histo), norm=LogNorm())\n",
    "\n",
    "# tornado mask bounding curves\n",
    "#for i in range(-shift_n_times, shift_n_times+1):\n",
    "#    l1 = \"simulation\" if i == 0 else \"\"\n",
    "#    l2 = \"shifted by 25ns\" if i == 0 else \"\"\n",
    "#    plt.plot(data[\"Timewalk\"]+i*shift_offset*tof_scaling, data[\"Qel \"], label=l1, c=\"b\")\n",
    "#    plt.plot(data[\"timewalk_shifted\"]+i*shift_offset*tof_scaling, data[\"Qel \"], label=l2, c=\"r\")\n",
    "\n",
    "#tornado mask\n",
    "#mask = plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(cell_mask==-1, cell_mask), alpha=.1, cmap=plt.get_cmap('Greys_r'))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time of flight [s]\", fontsize=16)\n",
    "plt.ylabel(\"Q [e-]\", fontsize=16)\n",
    "#plt.ylim(0,80000)\n",
    "plt.xlim(tof_low, tof_high)\n",
    "cbar = fig.colorbar(data_plot) \n",
    "cbar.set_label('Data count',size=18)\n",
    "plt.grid()\n",
    "\n",
    "plt.title(\"All hits [ {} hits| 100%]\".format(int(np.sum(tof_histo))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply mask to data and produce 3 plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(cell_mask.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cell_mask, origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_hits = src.mcOverlay(cell_mask, tof_above_histo)\n",
    "beta_hits = tof_above_histo - alpha_hits\n",
    "gamma_hits = tof_below_histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "data_plot = plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(alpha_hits==0, alpha_hits),\n",
    "                           vmin=1, vmax=np.max(tof_histo), norm=LogNorm())\n",
    "\n",
    "# tornado mask bounding curves\n",
    "#plt.plot(data[\"Timewalk\"], data[\"Qel \"], label=\"simulation\", c=\"b\")\n",
    "#plt.plot(data[\"timewalk_shifted\"], data[\"Qel \"], label=\"shifted by 25ns\", c=\"r\")\n",
    "\n",
    "#tornado mask\n",
    "#mask = plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(cell_mask==0, cell_mask), alpha=.6, cmap=plt.get_cmap('viridis'))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time of flight [s]\", fontsize=16)\n",
    "plt.ylabel(\"Q [e-]\", fontsize=16)\n",
    "#plt.ylim(0,80000)\n",
    "plt.xlim(tof_low, tof_high)\n",
    "cbar = fig.colorbar(data_plot) \n",
    "cbar.set_label('Data count',size=18)\n",
    "plt.grid()\n",
    "\n",
    "plt.title(\"Alpha hits [ {} hits| {}%]\".format(int(np.sum(alpha_hits)),\n",
    "                                              round(100*np.sum(alpha_hits)/np.sum(tof_histo), 2)))\n",
    "\n",
    "#plt.savefig(\"alpha_hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib\n",
    "\n",
    "#data\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "data_plot = plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(beta_hits<1e-8, beta_hits),\n",
    "                           vmin=1, vmax=np.max(tof_histo), norm=LogNorm())\n",
    "\n",
    "# tornado mask bounding curves\n",
    "#plt.plot(data[\"Timewalk\"], data[\"Qel \"], label=\"simulation\", c=\"b\")\n",
    "#plt.plot(data[\"timewalk_shifted\"], data[\"Qel \"], label=\"shifted by 25ns\", c=\"r\")\n",
    "\n",
    "#tornado mask\n",
    "#mask = plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(cell_mask==0, cell_mask), alpha=.6, cmap=plt.get_cmap('viridis'))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time of flight [s]\", fontsize=16)\n",
    "plt.ylabel(\"Q [e-]\", fontsize=16)\n",
    "#plt.ylim(0,80000)\n",
    "plt.xlim(tof_low, tof_high)\n",
    "cbar = fig.colorbar(data_plot) \n",
    "cbar.set_label('Data count',size=18)\n",
    "plt.grid()\n",
    "\n",
    "plt.title(\"Beta hits [ {} hits| {}%]\".format(int(np.sum(beta_hits)),\n",
    "                                             round(100*np.sum(beta_hits)/np.sum(tof_histo), 2)))\n",
    "\n",
    "#plt.savefig(\"beta_hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib\n",
    "\n",
    "#data\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "data_plot = plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(gamma_hits<1e-8, gamma_hits),\n",
    "                           vmin=1, vmax=np.max(tof_histo), norm=LogNorm())\n",
    "\n",
    "# tornado mask bounding curves\n",
    "#plt.plot(data[\"Timewalk\"], data[\"Qel \"], label=\"simulation\", c=\"b\")\n",
    "#plt.plot(data[\"timewalk_shifted\"], data[\"Qel \"], label=\"shifted by 25ns\", c=\"r\")\n",
    "\n",
    "#tornado mask\n",
    "#mask = plt.pcolormesh(X_mask, Y_mask, np.ma.masked_where(cell_mask==0, cell_mask), alpha=.6, cmap=plt.get_cmap('viridis'))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time of flight [s]\", fontsize=16)\n",
    "plt.ylabel(\"Q [e-]\", fontsize=16)\n",
    "#plt.ylim(0,80000)\n",
    "plt.xlim(tof_low, tof_high)\n",
    "cbar = fig.colorbar(data_plot) \n",
    "cbar.set_label('Data count',size=18)\n",
    "plt.grid()\n",
    "\n",
    "plt.title(\"Gamma hits [ {} hits| {}%]\".format(int(np.sum(gamma_hits)),\n",
    "                                              round(100*np.sum(gamma_hits)/np.sum(tof_histo), 2)))\n",
    "\n",
    "#plt.savefig(\"gamma_hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
